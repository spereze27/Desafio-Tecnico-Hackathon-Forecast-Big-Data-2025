# Desaf√≠o T√©cnico ‚Äì Hackathon Forecast Big Data 2025
## Proyecto de Predicci√≥n de Ventas para el Sector Retail

### Resumen del Proyecto

Este repositorio contiene la soluci√≥n desarrollada para el **Hackathon Forecast Big Data 2025**. El proyecto consiste en la creaci√≥n de un modelo de machine learning para predecir la cantidad de ventas semanales por producto (SKU) y por punto de venta (PDV). Utilizando un historial de transacciones, registros de productos e informaci√≥n de los PDVs correspondientes al a√±o 2022, el modelo genera predicciones de demanda para las cinco primeras semanas de enero de 2023.

La soluci√≥n busca abordar un desaf√≠o real del sector retail, inspirado en el producto **One-Click Order**, con el fin de optimizar la gesti√≥n de inventario y el proceso de reposici√≥n de productos.

---

### üéØ Objetivo

El objetivo principal de este proyecto es **desarrollar un modelo de *pron√≥stico* (predicci√≥n) preciso y robusto** que estime la cantidad de ventas semanales por Punto de Venta (PDV) y SKU para las cinco primeras semanas de enero de 2023.

Las predicciones generadas tienen como finalidad proporcionar una herramienta de apoyo a la decisi√≥n para el sector retail, permitiendo:

-   **Optimizar la gesti√≥n de inventario:** Garantizar que cada PDV tenga la cantidad ideal de cada producto.
-   **Automatizar la reposici√≥n:** Facilitar la creaci√≥n de pedidos de reposici√≥n de forma autom√°tica e inteligente.
-   **Reducir costos operativos:** Minimizar tanto el exceso de inventario (costos de almacenamiento) como las roturas de stock (p√©rdida de ventas por falta de producto).


---

### ‚öôÔ∏è Metodolog√≠a

La soluci√≥n se desarroll√≥ siguiendo un flujo de trabajo estructurado, comenzando con el entendimiento y preparaci√≥n de los datos, hasta la construcci√≥n y entrenamiento del modelo de pron√≥stico.

#### 1. Perfilado y An√°lisis Exploratorio de Datos (EDA)

El primer paso consisti√≥ en realizar un **perfilado de datos** (`data profiling`) para inspeccionar en detalle los archivos Parquet proporcionados.

* **¬øPor qu√© es importante?** El perfilado de datos es un paso fundamental que permite entender la estructura, calidad y contenido de los datos. Ayuda a identificar el esquema (nombre y tipo de las columnas), detectar valores nulos o inconsistentes, entender la distribuci√≥n de las variables y verificar las relaciones entre las tablas. Una comprensi√≥n clara de los datos desde el inicio es crucial para tomar decisiones informadas en las etapas de limpieza, ingenier√≠a de caracter√≠sticas y modelado.

#### 2. Elecci√≥n de la Tecnolog√≠a: Spark vs. Pandas/Polars

El conjunto de datos de transacciones contiene **m√°s de 6 millones de registros**, lo que lo clasifica como un problema de Big Data. Para procesar este volumen de informaci√≥n de manera eficiente, se eligi√≥ **Apache Spark**.

* **¬øPor qu√© Spark?** Mientras que librer√≠as como **Pandas** o **Polars** son extremadamente eficientes para conjuntos de datos que caben en la memoria RAM de una sola m√°quina (procesamiento de nodo √∫nico), **Spark** est√° dise√±ado para el **procesamiento distribuido**. Spark divide los datos y las operaciones a trav√©s de un cl√∫ster de m√∫ltiples m√°quinas, ejecutando tareas en paralelo. Esta arquitectura le permite escalar horizontalmente y manejar vol√∫menes de datos que superan con creces la capacidad de un solo computador, garantizando un rendimiento √≥ptimo para este desaf√≠o.

#### 3. Estructura y Relaci√≥n de los Datos

Los datos se encuentran distribuidos en tres tablas principales: `productos`, `pdvs` (puntos de venta) y `transacoes` (transacciones). La tabla `transacoes` act√∫a como la tabla de hechos, conect√°ndose con las otras dos a trav√©s de las claves for√°neas `produto` y `pdv`.

A continuaci√≥n, se presenta una vista previa del contenido de cada tabla.

---

### üõçÔ∏è Datos de Productos

Esta tabla contiene la informaci√≥n detallada de cada producto.

| produto             | categoria         | descricao                |
| ------------------- | ----------------- | ------------------------ |
| `2282334733936076502` | `Distilled Spirits` | `JOSEPH CARTRON CA...`   |
| `6091840953834683482` | `Distilled Spirits` | `SPRINGBANK 18 YEA...`   |
| `1968645851245092408` | `Distilled Spirits` | `J BRANDT TRIPLE S...`   |
| `994706710729219179`  | `Draft`             | `REFORMATION CASHM...`   |
| `9209550539540384349` | `Non-Alcohol`       | `HELLA MOSCOW MULE...`   |

---

### üõí Datos de Transacciones

Esta es la tabla de hechos que registra cada venta, vinculando un producto con un punto de venta.

| pdv                 | produto             | transaction_date | quantity |
| ------------------- | ------------------- | ---------------- | -------- |
| `7384367747233276219` | `328903483604537190`  | `2022-07-13`     | `1.0`      |
| `3536908514005606262` | `5418855670645487653` | `2022-03-21`     | `6.0`      |
| `3138231730993449825` | `1087005562675741887` | `2022-09-06`     | `3.0`      |
| `3681167389484217654` | `1401422983880045188` | `2022-09-11`     | `129.0`    |
| `7762413312337359369` | `6614994347738381720` | `2022-02-18`     | `1.0`      |

---

### üè™ Datos de PDVs (Puntos de Venta)

Aqu√≠ se almacena la informaci√≥n de los diferentes puntos de venta.

| pdv                 | premise     | categoria_pdv  | zipcode |
| ------------------- | ----------- | -------------- | ------- |
| `2204965430669363375` | `On Premise`  | `Mexican Rest` | `30741` |
| `5211957289528622910` | `On Premise`  | `Hotel/Motel`  | `80011` |
| `9024493554530757353` | `Off Premise` | `Convenience`  | `80751` |
| `8659197371382902429` | `On Premise`  | `Restaurant`   | `80439` |
| `1400854873763881130` | `On Premise`  | `Restaurant`   | `30093` |

A continuaci√≥n se muestra el esquema de la relaci√≥n entre las tablas:

![Diagrama de relaci√≥n de tablas de la base de datos](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/union_tablas.png)

## üßπ Preprocesamiento B√°sico

Antes de realizar el an√°lisis y modelado, se llev√≥ a cabo un preprocesamiento b√°sico de los datos para asegurar su calidad y consistencia. Este proceso se dividi√≥ en tres pasos principales:

### 1. Gesti√≥n de Valores Nulos

Se identific√≥ la cantidad de valores nulos por cada columna.

-   La caracter√≠stica `label` era la √∫nica con una cantidad considerable de valores ausentes (aproximadamente un 8%). Para no perder estos registros, se decidi√≥ imputar estos valores asign√°ndoles la categor√≠a **`sem_label`**.
-   Para el resto de las columnas (`premise`, `categoria_pdv`, `zipcode` y `subcategoria`), que conten√≠an un porcentaje muy bajo de nulos (menos del 1%), se opt√≥ por eliminar las filas correspondientes.

A continuaci√≥n, se muestra el porcentaje de valores nulos por columna antes de la limpieza:

| pdv | produto | distributor_id | transaction_date | reference_date | quantity | gross_value | net_value | gross_profit | discount | taxes | categoria | descricao | tipos | label | subcategoria | marca | fabricante | premise | categoria_pdv | zipcode |
|:---:|:-------:|:--------------:|:----------------:|:--------------:|:--------:|:-----------:|:---------:|:------------:|:--------:|:-----:|:---------:|:---------:|:-----:|:-----:|:------------:|:-----:|:----------:|:-------:|:-------------:|:-------:|
| 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | **8.03%** | **0.16%** | 0.0 | 0.0 | **0.69%** | **0.69%** | **0.69%** |

---

### 2. Normalizaci√≥n de Variables Categ√≥ricas

Para evitar inconsistencias y duplicidad en las categor√≠as, se aplicaron las siguientes transformaciones a las columnas de tipo texto:

-   **Conversi√≥n a may√∫sculas:** Todas las cadenas de texto se convirtieron a may√∫sculas.
-   **Eliminaci√≥n de espacios:** Se eliminaron los espacios en blanco al inicio y al final de cada texto.

Esto asegura que categor√≠as id√©nticas pero con diferente formato (p. ej., `Specialty` y `specialty`) sean tratadas como una sola, unific√°ndolas como `SPECIALTY`.

---

### 3. Verificaci√≥n del Esquema de Datos

Finalmente, se revis√≥ que el tipo de dato de cada columna fuera coherente con los valores que conten√≠a. Por ejemplo, se verific√≥ que las columnas de fechas tuvieran el formato de fecha adecuado y que las columnas num√©ricas no contuvieran caracteres extra√±os.

## üìà An√°lisis de Ventas Semanales

Para entender la estacionalidad y el comportamiento de las ventas, se realiz√≥ un an√°lisis de la cantidad total de productos vendidos por cada semana del a√±o 2022.

### Comportamiento a lo Largo del A√±o

Al visualizar las ventas semanales, se observa un patr√≥n mayormente estable durante todo el a√±o, con una excepci√≥n notable: un **pico de ventas extremadamente alto** entre las semanas 30 y 40.

![Gr√°fico de ventas totales por semana en 2022](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/Captura%20desde%202025-09-21%2022-10-00.png)

Una investigaci√≥n m√°s profunda revel√≥ que este comportamiento an√≥malo se concentraba espec√≠ficamente en la **semana 36**.

---

### Investigaci√≥n de la Anomal√≠a en la Semana 36

Para determinar la causa del pico, se analiz√≥ si el aumento en las ventas se deb√≠a a un producto o a un punto de venta (PDV) espec√≠fico. Se agruparon las ventas de esa semana por PDV para identificar a los principales contribuyentes.

**Top 5 Puntos de Venta (PDV) por Ventas en la Semana 36**

| pdv                 | categoria_pdv  | sum(quantity)      |
| ------------------- | -------------- | ------------------ |
| `6491855528940268514` | `PACKAGE/LIQUOR` | 176,202.57         |
| `3025867614395044464` | `PACKAGE/LIQUOR` | 170,252.53         |
| `8723723113467008071` | `PACKAGE/LIQUOR` | 132,026.31         |
| `4304226119364518876` | `PACKAGE/LIQUOR` | 130,715.99         |
| `9171644843739559005` | `PACKAGE/LIQUOR` | 126,486.75         |

El an√°lisis demostr√≥ que el aumento **no se deb√≠a a un √∫nico producto o punto de venta**, sino que fue un comportamiento generalizado durante toda esa semana.
Dado que el comportamiento de la semana 36 es un **valor at√≠pico (outlier)** que no representa la tendencia general de ventas, se tom√≥ la decisi√≥n de **excluir todos los datos correspondientes a esa semana**. Esta medida se implement√≥ para evitar que esta anomal√≠a introdujera ruido y afectara negativamente el rendimiento del modelo de pron√≥stico.

## üìä Visualizaci√≥n de Ventas Post-Limpieza

Tras excluir la semana 36, el comportamiento de las ventas a lo largo del a√±o se puede apreciar con mayor claridad. La nueva gr√°fica muestra una tendencia mucho m√°s coherente y representativa de la estacionalidad del negocio.

![Gr√°fico de ventas totales por semana en 2022 sin la semana 36](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/Captura%20desde%202025-09-21%2022-21-49.png)

### Observaciones Clave

Al eliminar el valor at√≠pico, se pueden identificar patrones claros en los datos:

1.  **Tendencia Creciente:** Se observa una tendencia general al alza en las ventas a medida que avanza el a√±o.
2.  **Picos de Fin de A√±o:** Las ventas alcanzan su punto m√°ximo durante el √∫ltimo trimestre del a√±o (semanas 45-52), lo cual es consistente con el aumento de la demanda durante las temporadas festivas.
3.  **Estacionalidad Visible:** La gr√°fica ahora revela fluctuaciones y picos estacionales que antes eran opacados por la anomal√≠a, proporcionando una base mucho m√°s fiable para el pron√≥stico.

Con un conjunto de datos limpio y representativo, podemos proceder con la construcci√≥n de un modelo de forecasting m√°s preciso y robusto.

## üî¨ An√°lisis por Caracter√≠sticas

Una vez limpios los datos, se realiz√≥ un an√°lisis exploratorio segmentando las ventas por diferentes caracter√≠sticas para entender mejor los factores que influyen en la demanda.

### Ventas por Tipo de Producto (Label)

Al analizar las ventas semanales agrupadas por el `label` del producto, se observa un claro dominador.

-   **Productos `CORE`:** Esta categor√≠a representa la gran mayor√≠a de las ventas y mantiene su liderazgo de manera consistente a lo largo de todo el a√±o.
-   **Otras categor√≠as:** Las dem√°s etiquetas como `SEM_LABEL`, `DISCONTINUED`, `ALLOCATED` y `INGOUT` tienen un volumen de ventas significativamente menor.

![Ventas semanales por los 5 tipos de producto principales](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/producto%20mas%20vendido.png)

---

### Ventas por Categor√≠a de Producto

El an√°lisis de las categor√≠as de producto revela una concentraci√≥n de las ventas en un solo segmento.

-   **Categor√≠a `PACKAGE`:** Es, por un amplio margen, la categor√≠a con mayor cantidad de unidades vendidas, superando con creces a las dem√°s.
-   **Otras categor√≠as relevantes:** `DISTILLED SPIRITS` y `NON-ALCOHOL` le siguen en importancia, aunque con un volumen mucho menor.

![Top 10 categor√≠as de producto por cantidad vendida](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/categoria%20mas%20vendida.png)

---

### Relaci√≥n entre Descuento y Cantidad Vendida

Finalmente, se explor√≥ la relaci√≥n entre el descuento aplicado y la cantidad vendida para identificar patrones de compra. Durante este an√°lisis, se hizo un descubrimiento importante.

-   **Detecci√≥n de Valores Negativos:** El gr√°fico de dispersi√≥n mostr√≥ que exist√≠an registros con una **cantidad vendida negativa**. Estos valores at√≠picos probablemente corresponden a **devoluciones, reembolsos o ajustes de inventario**.

![Gr√°fico de dispersi√≥n de descuento vs cantidad vendida](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/dispersion%20con%20reembolsos.png)


El objetivo de este proyecto es construir un modelo para **predecir la demanda futura (ventas)**, no para pronosticar devoluciones. Por lo tanto, se tom√≥ la decisi√≥n de **eliminar todos los registros con cantidades negativas**. Esto evita introducir ruido en el modelo y le permite generalizar mejor los patrones de compra reales.

Una vez eliminados los productos reembolsados se tiene la siguiente dispersi√≥n en donde se  muestra que la mayoria de clientes compra una cantidad reducida de productos con un descuento moderado pero que a medida que se aumenta el descuento se tienen compras en grandes cantidades de un producto.

![Gr√°fico de dispersi√≥n de descuento vs cantidad vendida](https://github.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/blob/main/Captura%20desde%202025-09-21%2023-27-26.png)

## üì¶ Distribuci√≥n de Ventas por D√≠a de la Semana

Para completar el an√°lisis exploratorio, se examin√≥ c√≥mo se distribuyen las ventas a lo largo de los d√≠as de la semana. Esta visualizaci√≥n nos permite identificar qu√© d√≠as son comercialmente m√°s fuertes.

![Distribuci√≥n de Ventas por D√≠a de la Semana](https://raw.githubusercontent.com/spereze27/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/main/Captura%20desde%202025-09-21%2023-23-20.png)

### Interpretaci√≥n del Gr√°fico

El diagrama de cajas y bigotes revela un patr√≥n claro en el comportamiento de compra semanal:

-   **D√≠as de Menor Actividad:** Se observa que los d√≠as en medio de la semana, espec√≠ficamente **mi√©rcoles, jueves y viernes**, presentan una mediana de ventas inferior en comparaci√≥n con el resto de los d√≠as. La caja, que representa el 50% central de los datos, tambi√©n se encuentra en un rango m√°s bajo.
-   **D√≠as de Mayor Actividad:** Por el contrario, el inicio de la semana (**lunes y martes**) y el fin de semana (**s√°bado y domingo**) muestran una distribuci√≥n de ventas m√°s alta y con mayor dispersi√≥n, indicando un volumen de transacciones superior.

Esta informaci√≥n es valiosa para el modelo, ya que confirma que el **d√≠a de la semana es una caracter√≠stica importante** que influye directamente en la cantidad de productos vendidos.

## üß† Modelado y Pron√≥stico

Con los datos limpios y las caracter√≠sticas dise√±adas, el siguiente paso es construir, entrenar y evaluar un modelo de Machine Learning capaz de pronosticar la demanda semanal.

---

### 1. Ingenier√≠a de Caracter√≠sticas (Feature Engineering)

El rendimiento del modelo depende directamente de la calidad de las caracter√≠sticas (features) que se le proporcionen. Se crearon varias features para capturar diferentes aspectos del comportamiento de las ventas:

-   **Agregaci√≥n Temporal:** La base del modelo es la agregaci√≥n de las transacciones a nivel semanal por cada combinaci√≥n de `pdv` (punto de venta) y `produto`.
-   **Features de Ventana M√≥vil (Lag & Rolling Average):** Se crearon caracter√≠sticas de `lag` (ventas de 1, 2 y 4 semanas anteriores) y promedios m√≥viles. Estas features son **cruciales** para los modelos de series temporales, ya que le informan al modelo sobre la tendencia y el momentum de las ventas recientes.
-   **Features de Calendario:** Se extrajeron caracter√≠sticas como el `mes` y el `trimestre`. Adem√°s, se cre√≥ una feature binaria (`es_semana_importante`) que marca las semanas que contienen festivos o eventos comerciales importantes (como Black Friday o Navidad), ayudando al modelo a aprender sobre la estacionalidad y los picos de demanda.
-   **Features Descriptivas y de Precio:** Se reincorporaron las caracter√≠sticas categ√≥ricas del producto y del punto de venta para darle contexto al modelo. Tambi√©n se calcul√≥ el `precio_promedio_prod` para que el modelo pueda inferir la relaci√≥n entre el precio y la cantidad vendida.

---

### 2. Construcci√≥n del Pipeline de Machine Learning

Para asegurar que todas las transformaciones se apliquen de manera consistente tanto en el entrenamiento como en la predicci√≥n, se utiliz√≥ un **`Pipeline` de PySpark**. Este pipeline define el flujo de trabajo completo:

1.  **`StringIndexer`:** Convierte todas las columnas categ√≥ricas (como `categoria` o `label`) en √≠ndices num√©ricos, ya que los modelos de machine learning operan con n√∫meros.
2.  **`FeatureHasher`:** Toma los m√∫ltiples √≠ndices categ√≥ricos y los convierte en un √∫nico vector de caracter√≠sticas de tama√±o fijo (1024 en este caso). Esta es una t√©cnica eficiente para manejar una gran cantidad de categor√≠as sin crear una cantidad excesiva de columnas.
3.  **`VectorAssembler`:** Une el vector de caracter√≠sticas categ√≥ricas (del `FeatureHasher`) con todas las caracter√≠sticas num√©ricas (lags, promedios, calendario, etc.) en un √∫nico vector llamado `"features"`.
4.  **`GBTRegressor`:** El estimador final, que tomar√° el vector `"features"` para predecir la cantidad de ventas.

---

### 3. Elecci√≥n del Modelo: Gradient Boosted Trees (GBT)

Se eligi√≥ el modelo **Gradient Boosted Trees (GBT)** por varias razones estrat√©gicas:

-   **Alto Rendimiento:** Los modelos basados en √°rboles de decisi√≥n, y en particular los ensambles como GBT, son conocidos por su excelente rendimiento en datos tabulares como este, ya que pueden capturar relaciones complejas y no lineales entre las caracter√≠sticas.
-   **Robustez:** GBT es robusto frente a valores at√≠picos y no requiere que las caracter√≠sticas num√©ricas est√©n escaladas, lo que simplifica el preprocesamiento.
-   **Manejo de Interacciones:** Captura de forma natural las interacciones entre diferentes caracter√≠sticas. Por ejemplo, podr√≠a aprender que un `label` espec√≠fico se vende m√°s en una `categoria_pdv` particular solo durante un `mes` concreto.
-   **Escalabilidad:** La implementaci√≥n de GBT en Spark est√° dise√±ada para ejecutarse de manera distribuida en grandes vol√∫menes de datos, lo que es ideal para un contexto de Big Data.

---

### 4. Entrenamiento y Optimizaci√≥n del Modelo

El proceso de entrenamiento se realiz√≥ de la siguiente manera:

1.  **Divisi√≥n Temporal de Datos:** Se realiz√≥ una divisi√≥n **temporal** de los datos. El modelo se entren√≥ con los datos hasta la semana 40 (`train_data`) y se evalu√≥ su rendimiento en las semanas 41 a 49 (`test_data`). Esta divisi√≥n es fundamental en problemas de series temporales para simular un escenario real donde se predice el futuro bas√°ndose en el pasado.
2.  **Optimizaci√≥n de Hiperpar√°metros:** En lugar de elegir manualmente los par√°metros del modelo (como la profundidad de los √°rboles), se utiliz√≥ un **`CrossValidator`** con una grilla de par√°metros (`ParamGridBuilder`). Este proceso prueba autom√°ticamente m√∫ltiples combinaciones de hiperpar√°metros en una muestra de los datos de entrenamiento y selecciona la que ofrece el mejor rendimiento (menor RMSE), asegurando que el modelo est√© bien calibrado.
3.  **Entrenamiento Final:** Una vez encontrados los mejores hiperpar√°metros, se entren√≥ el pipeline final utilizando el **conjunto completo de datos de entrenamiento**.

---

### 5. Generaci√≥n de Predicciones para Enero 2023

El paso final es utilizar el modelo entrenado para pronosticar las ventas de las primeras 5 semanas de 2023. Para ello, se tuvo que construir un DataFrame con la estructura que el modelo espera.

#### ¬øC√≥mo se seleccionaron los productos a predecir?

No tiene sentido predecir las ventas de todas las combinaciones posibles de producto-tienda, ya que muchas pueden estar inactivas. Se aplic√≥ un filtro basado en la l√≥gica de negocio para enfocar el pron√≥stico solo en los productos **relevantes y activos**:

1.  **Filtro por Actividad Reciente:** Primero, se seleccionaron √∫nicamente las ventas ocurridas despu√©s de la semana 42 de 2022. Esto asegura que solo se consideren los productos y tiendas que han tenido **actividad en los √∫ltimos dos meses del a√±o**.
2.  **Filtro por Frecuencia:** Sobre este subconjunto reciente, se agruparon los datos por `pdv` y `produto` y se cont√≥ en cu√°ntas semanas distintas se vendi√≥ cada uno. Se conservaron √∫nicamente aquellas combinaciones que tuvieron ventas en **al menos 2 semanas diferentes** durante este per√≠odo reciente.

Este doble filtro es clave porque **a√≠sla las combinaciones de producto-tienda que tienen una actividad comercial constante y reciente**, eliminando productos espor√°dicos o descatalogados. Esto hace que las predicciones sean m√°s estables, relevantes y computacionalmente eficientes.

Finalmente, para estas combinaciones filtradas, se generaron las caracter√≠sticas de lag y promedios m√≥viles **utilizando los datos de las √∫ltimas semanas de 2022** como una aproximaci√≥n de c√≥mo empezar√≠an en 2023, permitiendo as√≠ que el modelo realizara las predicciones.