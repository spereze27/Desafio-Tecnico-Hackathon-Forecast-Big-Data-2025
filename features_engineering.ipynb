{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed087bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/16 11:12:34 WARN Utils: Your hostname, QUIN-DAT-A0012, resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp2s0)\n",
      "25/09/16 11:12:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/16 11:12:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession creada exitosamente!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HackathonForecast\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession creada exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos limpios desde la capa Silver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando transacciones a nivel semanal...\n",
      "Reincorporando las features descriptivas...\n",
      "Creando features de Lag y Ventana Móvil...\n",
      "Creando features de calendario...\n",
      "\n",
      "¡Ingeniería de Features completada! (Ahora con todas las columnas)\n",
      "Muestra del DataFrame final:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                (0 + 16) / 16][Stage 5:>                  (0 + 0) / 6]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, avg, stddev, col, sum, weekofyear, month, floor\n",
    "\n",
    "# --- 1. CARGAR DATOS LIMPIOS DE LA CAPA SILVER ---\n",
    "base_path = \"/home/quind/GIT/Desafio-Tecnico-Hackathon-Forecast-Big-Data-2025/\"\n",
    "silver_path = f\"{base_path}silver/datos_limpios\"\n",
    "\n",
    "print(\"Cargando datos limpios desde la capa Silver...\")\n",
    "df_final = spark.read.parquet(silver_path)\n",
    "\n",
    "# --- 2. CREAR TABLAS DE DIMENSIONES (DESCRIPTIVAS) ---\n",
    "# Seleccionamos las características únicas de cada producto\n",
    "features_produto = df_final.select(\"produto\", \"categoria\", \"label\", \"subcategoria\", \"marca\").distinct()\n",
    "\n",
    "# Seleccionamos las características únicas de cada PDV\n",
    "# CAMBIO: Se ha eliminado \"zipcode\" de la selección de características\n",
    "features_pdv = df_final.select(\"pdv\", \"premise\", \"categoria_pdv\").distinct()\n",
    "\n",
    "# --- 3. AGREGACIÓN SEMANAL ---\n",
    "print(\"Agregando transacciones a nivel semanal...\")\n",
    "df_semanal = df_final.groupBy(\"pdv\", \"produto\", weekofyear(\"transaction_date\").alias(\"semana\")) \\\n",
    "    .agg(sum(\"quantity\").alias(\"cantidad_total_semanal\"))\n",
    "\n",
    "# --- 4. ENRIQUECER DATOS SEMANALES CON LAS DIMENSIONES ---\n",
    "print(\"Reincorporando las features descriptivas...\")\n",
    "df_enriquecido = df_semanal.join(features_produto, \"produto\", \"left\") \\\n",
    "                           .join(features_pdv, \"pdv\", \"left\")\n",
    "\n",
    "# --- 5. CREACIÓN DE FEATURES DE LAG Y VENTANA MÓVIL ---\n",
    "print(\"Creando features de Lag y Ventana Móvil...\")\n",
    "# Se recomienda reparticionar antes de la ventana para optimizar el rendimiento\n",
    "windowSpec = Window.partitionBy(\"pdv\", \"produto\").orderBy(\"semana\")\n",
    "\n",
    "df_con_features = df_enriquecido \\\n",
    "    .withColumn(\"lag_1\", lag(\"cantidad_total_semanal\", 1, 0).over(windowSpec)) \\\n",
    "    .withColumn(\"lag_2\", lag(\"cantidad_total_semanal\", 2, 0).over(windowSpec)) \\\n",
    "    .withColumn(\"lag_4\", lag(\"cantidad_total_semanal\", 4, 0).over(windowSpec)) \\\n",
    "    .withColumn(\"media_movil_4_semanas\", avg(\"cantidad_total_semanal\").over(windowSpec.rowsBetween(-3, 0))) \\\n",
    "    .withColumn(\"stddev_movil_4_semanas\", stddev(\"cantidad_total_semanal\").over(windowSpec.rowsBetween(-3, 0)))\n",
    "\n",
    "# --- 6. CREACIÓN DE FEATURES DE CALENDARIO ---\n",
    "print(\"Creando features de calendario...\")\n",
    "df_con_features = df_con_features.withColumn(\"mes\", floor((col(\"semana\") - 1) / 4.34) + 1)\n",
    "\n",
    "df_listo_para_modelo = df_con_features.fillna(0)\n",
    "\n",
    "print(\"\\n¡Ingeniería de Features completada! (Ahora con todas las columnas)\")\n",
    "print(\"Muestra del DataFrame final:\")\n",
    "df_listo_para_modelo.show()\n",
    "\n",
    "print(\"\\nEsquema final del DataFrame:\")\n",
    "df_listo_para_modelo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividiendo los datos en entrenamiento y prueba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de entrenamiento: 4296393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas de prueba: 1424401\n",
      "\n",
      "Entrenando el pipeline del modelo ML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/15 20:15:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/09/15 20:15:55 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/09/15 20:15:56 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/09/15 20:16:07 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/09/15 20:16:15 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/09/15 20:16:22 WARN MemoryStore: Not enough space to cache rdd_426_6 in memory! (computed 413.6 MiB so far)\n",
      "25/09/15 20:16:22 WARN BlockManager: Persisting block rdd_426_6 to disk instead.\n",
      "25/09/15 20:16:23 WARN MemoryStore: Not enough space to cache rdd_426_15 in memory! (computed 413.6 MiB so far)\n",
      "25/09/15 20:16:23 WARN BlockManager: Persisting block rdd_426_15 to disk instead.\n",
      "25/09/15 20:16:23 WARN MemoryStore: Not enough space to cache rdd_426_13 in memory! (computed 413.6 MiB so far)\n",
      "25/09/15 20:16:23 WARN BlockManager: Persisting block rdd_426_13 to disk instead.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, FeatureHasher\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- 1. IDENTIFICAR TIPOS DE FEATURES ---\n",
    "# La variable objetivo que queremos predecir\n",
    "TARGET_COL = \"cantidad_total_semanal\"\n",
    "\n",
    "# Columnas categóricas que necesitamos codificar\n",
    "CATEGORICAL_COLS = [\n",
    "    \"pdv\", \"produto\", \"categoria\", \"label\", \"subcategoria\", \n",
    "    \"marca\", \"premise\", \"categoria_pdv\"\n",
    "]\n",
    "\n",
    "# Columnas numéricas que ya están listas para usar\n",
    "# CAMBIO: Se ha eliminado \"zipcode\" de la lista\n",
    "NUMERICAL_COLS = [\n",
    "    \"semana\", \"lag_1\", \"lag_2\", \"lag_4\", \n",
    "    \"media_movil_4_semanas\", \"stddev_movil_4_semanas\", \"mes\"\n",
    "]\n",
    "\n",
    "# --- 2. DEFINIR LAS ETAPAS DEL PIPELINE ---\n",
    "\n",
    "# Etapa 1: StringIndexer - Convierte texto a índices numéricos\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in CATEGORICAL_COLS\n",
    "]\n",
    "# Guardamos los nombres de las columnas de salida del indexer\n",
    "indexed_cols = [f\"{c}_idx\" for c in CATEGORICAL_COLS]\n",
    "\n",
    "# CAMBIO: Etapa 2: FeatureHasher en lugar de OneHotEncoder\n",
    "# Convierte múltiples columnas categóricas indexadas en un solo vector de características.\n",
    "# Es mucho más eficiente en memoria. Puedes ajustar 'numFeatures' según sea necesario.\n",
    "hasher = FeatureHasher(\n",
    "    inputCols=indexed_cols,\n",
    "    outputCol=\"hashed_features\",\n",
    "    numFeatures=1024  # Potencia de 2, ajusta este valor si es necesario\n",
    ")\n",
    "\n",
    "# Etapa 3: VectorAssembler - Une todas las features en un solo vector\n",
    "# CAMBIO: Ahora usamos la salida del hasher y las columnas numéricas\n",
    "feature_sources = [\"hashed_features\"] + NUMERICAL_COLS\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_sources,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Etapa 4: Modelo - Usaremos Gradient Boosted Trees Regressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=TARGET_COL)\n",
    "\n",
    "# CAMBIO: Unimos las nuevas etapas en el Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [hasher, assembler, gbt])\n",
    "\n",
    "# --- 3. DIVIDIR LOS DATOS (ENTRENAMIENTO Y PRUEBA) ---\n",
    "# Para series temporales, la división DEBE ser cronológica.\n",
    "print(\"Dividiendo los datos en entrenamiento y prueba...\")\n",
    "train_data = df_listo_para_modelo.filter(col(\"semana\") <= 40)\n",
    "test_data = df_listo_para_modelo.filter(col(\"semana\") > 40)\n",
    "\n",
    "# (Opcional) Cachear los dataframes de entrenamiento y prueba si tienes memoria suficiente\n",
    "# train_data.cache()\n",
    "# test_data.cache()\n",
    "\n",
    "print(f\"Filas de entrenamiento: {train_data.count()}\")\n",
    "print(f\"Filas de prueba: {test_data.count()}\")\n",
    "\n",
    "# --- 4. ENTRENAR EL MODELO ---\n",
    "print(\"\\nEntrenando el pipeline del modelo ML...\")\n",
    "model = pipeline.fit(train_data)\n",
    "print(\"¡Entrenamiento completado!\")\n",
    "\n",
    "# --- 5. REALIZAR PREDICCIONES Y EVALUAR ---\n",
    "print(\"\\nRealizando predicciones en el conjunto de prueba...\")\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Mostrar algunas predicciones\n",
    "predictions.select(\"pdv\", \"produto\", \"semana\", TARGET_COL, \"prediction\").show(10)\n",
    "\n",
    "# Evaluar el rendimiento del modelo usando RMSE (Error Cuadrático Medio Raíz)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=TARGET_COL,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"\\nRoot Mean Squared Error (RMSE) en los datos de prueba = {rmse}\")\n",
    "\n",
    "# (Opcional) Limpiar el cache\n",
    "# train_data.unpersist()\n",
    "# test_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5cabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, round\n",
    "\n",
    "# --- 1. RE-ENTRENAMIENTO DEL MODELO FINAL CON TODOS LOS DATOS DE 2022 ---\n",
    "print(\"Re-entrenando el pipeline con todos los datos de 2022...\")\n",
    "# 'pipeline' es el pipeline que definiste en el paso anterior\n",
    "# 'df_listo_para_modelo' es tu DataFrame con todas las features de 2022\n",
    "final_model = pipeline.fit(df_listo_para_modelo)\n",
    "print(\"Modelo final entrenado.\")\n",
    "\n",
    "# --- 2. GENERACIÓN ITERATIVA DE PREDICCIONES PARA ENERO 2023 ---\n",
    "\n",
    "# Preparar los datos históricos (todo 2022) para calcular los primeros lags\n",
    "datos_historicos = df_listo_para_modelo\n",
    "\n",
    "# Obtener la lista única de PDV/Producto para los que necesitamos predecir\n",
    "pdv_produto_unicos = df_listo_para_modelo.select(\"pdv\", \"produto\").distinct()\n",
    "\n",
    "# Lista para guardar los DataFrames de predicción de cada semana\n",
    "predicciones_finales = []\n",
    "\n",
    "print(\"\\nIniciando bucle de predicción para las 5 semanas de Enero/2023...\")\n",
    "for i in range(1, 6):\n",
    "    semana_a_predecir = 52 + i # Las semanas 53, 54, 55, 56, 57\n",
    "    print(f\"--- Prediciendo para la semana {i}/5 (semana continua {semana_a_predecir}) ---\")\n",
    "\n",
    "    # A. Crear el DataFrame para la semana que queremos predecir\n",
    "    df_pred_semana_actual = pdv_produto_unicos.withColumn(\"semana\", lit(semana_a_predecir))\n",
    "\n",
    "    # B. Enriquecer con features descriptivas (label, categoria, etc.)\n",
    "    # (Reutilizamos el código del notebook anterior para esto)\n",
    "    features_produto = df_final.select(\"produto\", \"categoria\", \"label\", \"subcategoria\", \"marca\").distinct()\n",
    "    features_pdv = df_final.select(\"pdv\", \"premise\", \"categoria_pdv\", \"zipcode\").distinct()\n",
    "    \n",
    "    df_pred_semana_actual = df_pred_semana_actual.join(features_produto, \"produto\", \"left\") \\\n",
    "                                                 .join(features_pdv, \"pdv\", \"left\")\n",
    "\n",
    "    # C. Crear features de lag y ventana móvil usando los datos históricos\n",
    "    windowSpec = Window.partitionBy(\"pdv\", \"produto\").orderBy(\"semana\")\n",
    "    df_para_predecir_features = df_pred_semana_actual.unionByName(datos_historicos.select(df_pred_semana_actual.columns), allowMissingColumns=True) \\\n",
    "        .withColumn(\"lag_1\", lag(\"cantidad_total_semanal\", 1, 0).over(windowSpec)) \\\n",
    "        .withColumn(\"lag_2\", lag(\"cantidad_total_semanal\", 2, 0).over(windowSpec)) \\\n",
    "        .withColumn(\"lag_4\", lag(\"cantidad_total_semanal\", 4, 0).over(windowSpec)) \\\n",
    "        .withColumn(\"media_movil_4_semanas\", avg(\"cantidad_total_semanal\").over(windowSpec.rowsBetween(-3, 0))) \\\n",
    "        .withColumn(\"stddev_movil_4_semanas\", stddev(\"cantidad_total_semanal\").over(windowSpec.rowsBetween(-3, 0))) \\\n",
    "        .withColumn(\"mes\", floor((col(\"semana\") - 1) / 4.34) + 1) \\\n",
    "        .filter(col(\"semana\") == semana_a_predecir) \\\n",
    "        .fillna(0)\n",
    "\n",
    "    # D. Realizar la predicción\n",
    "    prediccion_actual = final_model.transform(df_para_predecir_features)\n",
    "    \n",
    "    # E. Guardar la predicción de esta semana\n",
    "    prediccion_formateada = prediccion_actual.select(\n",
    "        col(\"semana\"),\n",
    "        col(\"pdv\"),\n",
    "        col(\"produto\"),\n",
    "        col(\"prediction\").alias(\"cantidad_total_semanal\") # Usamos la predicción como si fuera la venta real\n",
    "    )\n",
    "    predicciones_finales.append(prediccion_formateada)\n",
    "\n",
    "    # F. Actualizar los datos históricos con nuestra nueva predicción para el siguiente ciclo\n",
    "    datos_historicos = datos_historicos.unionByName(prediccion_formateada)\n",
    "\n",
    "# --- 3. FORMATEO Y GUARDADO DEL ARCHIVO DE SUBMISIÓN ---\n",
    "print(\"\\nBucle finalizado. Uniendo y formateando el archivo de submisión...\")\n",
    "\n",
    "# Unir las 5 semanas de predicciones en un solo DataFrame\n",
    "from functools import reduce\n",
    "df_submision = reduce(lambda df1, df2: df1.union(df2), predicciones_finales)\n",
    "\n",
    "# Formatear según las especificaciones del hackathon\n",
    "df_submision_final = df_submision \\\n",
    "    .withColumn(\"semana_real\", col(\"semana\") - 52) \\\n",
    "    .select(\n",
    "        col(\"semana_real\").alias(\"semana\"),\n",
    "        col(\"pdv\"),\n",
    "        col(\"produto\"),\n",
    "        # Asegurarse de que la cantidad no sea negativa, redondear y convertir a entero\n",
    "        when(col(\"cantidad_total_semanal\") < 0, 0)\n",
    "            .otherwise(round(col(\"cantidad_total_semanal\")))\n",
    "            .cast(\"integer\").alias(\"quantidade\")\n",
    "    )\n",
    "\n",
    "# Mostrar una muestra del archivo final\n",
    "df_submision_final.show(10)\n",
    "\n",
    "# Guardar el archivo en formato CSV con ';' como separador y sin header\n",
    "submission_path = f\"{base_path}submission\"\n",
    "print(f\"Guardando archivo de submisión en: {submission_path}\")\n",
    "\n",
    "df_submision_final.repartition(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(submission_path)\n",
    "\n",
    "print(\"¡Archivo de submisión generado exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
